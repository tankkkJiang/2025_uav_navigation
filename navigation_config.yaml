# ========== 世界参数 ==========
env_params:
  use_gui: False # 是否启用图形界面
  scene:
    type: voxelized
    voxel:
      size: 1 
    map:
      building_path: "assets/building/building.obj" # 建筑物模型路径
    obstacle:
      num_obstacles: 100 # 障碍物数量
      min_radius: 5 # 障碍物最小半径
      max_radius: 10 # 障碍物最大半径
      min_height: 60 # 障碍物最小高度
      max_height: 200 # 障碍物最大高度
    region:
      x_min: -250
      x_max: 250
      y_min: -250
      y_max: 250
      z_min: 0
      z_max: 200 # 仿真飞行区域范围
  drone:
    init_pos:  random # 无人机初始位置 (x, y, z)[0.0, 0.0, 60.0]
    urdf_path: "assets/cf2x.urdf" # 无人机模型路径
  action:
    type: horizon_discrete_adjust_3
    range: 1/8
  reward:
    extra_rewards:
      arrival_reward: 5.0 # 成功到达目标奖励
      collision_penalty: -5.0 # 碰撞惩罚
    active_components:
      # direction_reward: 1.0
      # spherical_direction_reward: 0.5
      # nonlinear_spherical_direction_reward: 0.5
      # nonlinear_collision_penalty: 1.0
      # linear_collision_penalty: 1.0
      # distance_to_obstacle_reward: 0.5
      # terminal_reward: 1.0
      # target_progress_reward: 0.8
      # velocity_reward: 0.3
      # image_nonlinear_collision_penalty: 1.0
      image_nonlinear_collision_penalty_2: 1.0
      # image_linear_collision_penalty: 1.0
      # cosine_spherical_direction_reward: 0.7
      # cosine_spherical_direction_reward_2: 0.7
      # tanh_spherical_direction_reward: 0.7
      # interpolation_spherical_direction_reward: 0.7
  episode:
    max_episode_timesteps: 200 # 每回合最大步数
  rollout:
    save_freq: 20
    num_test_episodes: 20
  observation:
    dim: 9
    grid_shape: (1,9)
    normalize: True
    to_real_distance: False
    features:
      # - spherical_velocity
      # - target_relative_position
      # - target_direction
      # - spherical_direction_error
  


algo_name: "PPO" # 可以选择 "PPO", "SAC", "TD3", "DDPG"
# ========== 模型学习参数 ==========
model_learn_params:
  total_timesteps: 500000 # 总训练步数
# ========== 特征提取器参数 ==========
feature_extractor_params:
  # activation_fn: nn.ReLU 
  feature_extractor: "concat"
  concat_output_dim: 16
  mobilenet_v2_output_dim: 16
  resnet_output_dim: 16 # ResNet编码输出维度
net_arch:
  pi: [64, 64]
  vf: [128, 128, 64]
# ========== PPO初始化参数 ==========
ppo_init_params:
  policy: "MlpPolicy" # 使用的策略网络
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.99
  clip_range: 0.2
  ent_coef: 0.01 # 增加策略熵奖励，引导更多探索
  vf_coef: 2      # 提高 critic 损失比重
  max_grad_norm: 0.5
  verbose: 1
  device: "cuda:1" # 指定使用第1块GPU
  seed: 0 # 随机种子（可复现）
# ------------- SAC初始化参数 -------------
sac_init_params:
  policy: "MultiInputPolicy"
  learning_starts: 2000
  learning_rate: 0.0003
  buffer_size: 1000000
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 200
  gradient_steps: 1
  ent_coef: "auto"
  verbose: 1
  device: "cuda:2"
  seed: 4
# ------------- TD3初始化参数 -------------
td3_init_params:
  policy: "MultiInputPolicy"
  learning_rate: 0.001
  buffer_size: 1000000
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  verbose: 1
  device: "cuda:0"
  seed: 42
# ------------- DDPG初始化参数 -------------
ddpg_init_params:
  policy: "MultiInputPolicy"
  learning_rate: 0.001
  buffer_size: 1000000
  batch_size: 256
  tau: 0.005
  gamma: 0.99
  train_freq: 1
  gradient_steps: 1
  verbose: 1
  device: "cuda:0"
  seed: 42
