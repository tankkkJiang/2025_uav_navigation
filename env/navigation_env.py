"""
env/low_level_env.py
"""

import gym
from gym import spaces
import numpy as np
import random
import logging
import yaml
from sim.world import World
from env.wrappers.reward_wrapper import (
    TargetProgressReward,
    NonlinearCollisionPenalty,
    DirectionReward,
    VelocityReward,
    LinearCollisionPenalty,
    TerminalReward,
    SphericalDirectionReward,
    DistanceToObstacleReward,
    NonlinearSphericalDirectionReward,
    ImageNonlinearCollisionPenalty,
    ImageNonlinearCollisionPenalty2,
    ImageLinearCollisionPenalty,
    CosineSphericalDirectionReward,
    CosineSphericalDirectionReward2,
    InterpolationSphericalDirectionReward,
    TanhSphericalDirectionReward
)

import pybullet as p
import wandb
import ast


class NavigationEnv(gym.Env):
    def __init__(self, env_params):
        super().__init__()
        self.env_params = env_params
        self._init_simulation()
        self._init_obs_space()
        self._init_action_space()
        self._init_reward()
        self._max_episode_steps = self.env_params['episode']['max_episode_timesteps']
        self._action_repeat = self.env_params['episode']['action_repeat']

        # åˆå§‹åŒ–æ¯ä¸ªç»„ä»¶çš„å›åˆç´¯è®¡å¥–åŠ±
        self.episode_total_reward = 0
        self.episode_component_rewards = {comp.name: 0.0 for comp in self.reward_components}
        

    def _init_simulation(self):
        scene_region = self.env_params['scene']['region']
        obstacle_params = self.env_params['scene']['obstacle']
        drone_params = self.env_params['drone']
        scene_type = self.env_params['scene'].get('type', 'random')
        voxel_size = self.env_params['scene'].get('voxel', {}).get('size', None)
        building_path = self.env_params.get('world', {}).get('building_path', '')

        self.sim = World(
            use_gui=self.env_params['use_gui'],
            scene_type=scene_type,
            scene_region=scene_region,
            obstacle_params=obstacle_params,
            drone_params=drone_params,
            voxel_size=voxel_size,
            building_path=building_path
        )

    def _init_action_space(self):
        """
        æ ¹æ®åŠ¨ä½œæ§åˆ¶æ¨¡å¼åˆå§‹åŒ–åŠ¨ä½œç©ºé—´ã€‚

        å‚æ•°:
            mode (str): 'cartesian', 'spherical', æˆ– 'adjust'
        """
        mode = self.env_params["action"]["type"]

        if mode == "cartesian":
            self.action_space = spaces.Box(
                low=np.array([-15.0, -15.0, -15.0], dtype=np.float32),
                high=np.array([15.0, 15.0, 15.0], dtype=np.float32),
                dtype=np.float32
            )

        elif mode == "spherical":
            self.action_space = spaces.Box(
                low=np.array([0.0, 0.0, -np.pi], dtype=np.float32),       # v âˆˆ [0, 15], Î¸ âˆˆ [0, Ï€], Ï† âˆˆ [-Ï€, Ï€]
                high=np.array([15.0, np.pi, np.pi], dtype=np.float32),
                dtype=np.float32
            )

        elif mode == "adjust":
            self.angle_range = eval(self.env_params["action"]["range"])
            self.action_space = spaces.Box(
                low=np.array([-self.angle_range*np.pi, -self.angle_range*np.pi], dtype=np.float32),  # v_abs, Î”Î¸, Î”Ï†
                high=np.array([self.angle_range*np.pi, self.angle_range*np.pi], dtype=np.float32),
                dtype=np.float32
            )
        elif mode == "discrete_adjust":
            self.action_space = spaces.Discrete(9)
            self.angle_range = eval(self.env_params["action"]["range"])
            # å®šä¹‰ç¦»æ•£åŠ¨ä½œæ˜ å°„è¡¨
            angle_options = [-self.angle_range*np.pi, 0.0, self.angle_range*np.pi]
            self.action_idx_to_delta = [
                (dx, dy)
                for dx in angle_options
                for dy in angle_options
            ]
        
        elif mode == "horizon_discrete_adjust_3":
            self.action_space = spaces.Discrete(3)
            self.angle_range = 1/8
            # å®šä¹‰ç¦»æ•£åŠ¨ä½œæ˜ å°„è¡¨
            angle_options = [-1/8*np.pi, 0.0, 1/8*np.pi]
            self.action_idx_to_delta = angle_options
        
        elif mode == "horizon_discrete_adjust_5":
            self.action_space = spaces.Discrete(5)
            # å®šä¹‰ç¦»æ•£åŠ¨ä½œæ˜ å°„è¡¨
            angle_options = [-1/4*np.pi, -1/8*np.pi, 0.0, 1/8*np.pi, 1/4*np.pi]
            self.action_idx_to_delta = angle_options
        
        elif mode == "horizon_discrete_adjust_7":
            self.action_space = spaces.Discrete(7)
            # å®šä¹‰ç¦»æ•£åŠ¨ä½œæ˜ å°„è¡¨
            angle_options = [-3/8*np.pi, -1/4*np.pi, -1/8*np.pi, 0.0, 1/8*np.pi, 1/4*np.pi, 3/8*np.pi]
            self.action_idx_to_delta = angle_options

        else:
            raise ValueError(f"Unsupported control mode: '{mode}'")

    def _init_obs_space(self):
        if "dim" in self.env_params.get("observation", {}):
            dim = self.env_params["observation"]["dim"]
            self.observation_space = spaces.Box(low=0, high=1, shape=(dim,), dtype=np.float32)
        else:
            self.observation_space = spaces.Box(low=0, high=1, shape=(16,), dtype=np.float32)

    def _init_reward(self):
        reward_params = self.env_params["reward"]
        self.reward_components = []
        active = reward_params["active_components"]  # dict: {name: weight}

        if "target_progress_reward" in active:
            self.reward_components.append(TargetProgressReward("target_progress", active["target_progress_reward"]))

        if "nonlinear_collision_penalty" in active:
            self.reward_components.append(NonlinearCollisionPenalty("nonlinear_collision_penalty", active["nonlinear_collision_penalty"]))

        if "linear_collision_penalty" in active:
            self.reward_components.append(LinearCollisionPenalty("linear_collision_penalty", active["linear_collision_penalty"]))

        if "distance_to_obstacle_reward" in active:
            self.reward_components.append(DistanceToObstacleReward("distance_to_obstacle_reward", active["distance_to_obstacle_reward"]))

        if "direction_reward" in active:
            self.reward_components.append(DirectionReward("direction_reward", active["direction_reward"]))

        if "spherical_direction_reward" in active:
            self.reward_components.append(SphericalDirectionReward("spherical_direction_reward", active["spherical_direction_reward"]))

        if "nonlinear_spherical_direction_reward" in active:
            self.reward_components.append(NonlinearSphericalDirectionReward("nonlinear_spherical_direction_reward", active["nonlinear_spherical_direction_reward"]))

        if "velocity_reward" in active:
            self.reward_components.append(VelocityReward("velocity_reward", active["velocity_reward"]))

        if "image_nonlinear_collision_penalty" in active:
            self.reward_components.append(
                ImageNonlinearCollisionPenalty("image_nonlinear_collision_penalty", active["image_nonlinear_collision_penalty"])
            )
        
        if "image_nonlinear_collision_penalty_2" in active:
            self.reward_components.append(
                ImageNonlinearCollisionPenalty2("image_nonlinear_collision_penalty_2", active["image_nonlinear_collision_penalty_2"])
            )

        if "image_linear_collision_penalty" in active:
            self.reward_components.append(
                ImageLinearCollisionPenalty("image_linear_collision_penalty", active["image_linear_collision_penalty"])
            )

        if "cosine_spherical_direction_reward" in active:
            self.reward_components.append(
                CosineSphericalDirectionReward("cosine_spherical_direction_reward", active["cosine_spherical_direction_reward"])
            )
        
        if "cosine_spherical_direction_reward_2" in active:
            self.reward_components.append(
                CosineSphericalDirectionReward2("cosine_spherical_direction_reward_2", active["cosine_spherical_direction_reward_2"])
            )

        if "tanh_spherical_direction_reward" in active:
            self.reward_components.append(
                TanhSphericalDirectionReward("tanh_spherical_direction_reward", active["tanh_spherical_direction_reward"])
            )
        
        if "interpolation_spherical_direction_reward" in active:
            self.reward_components.append(
                InterpolationSphericalDirectionReward("interpolation_spherical_direction_reward", active["interpolation_spherical_direction_reward"])
            )

        if "terminal_reward" in active:
            arrival_reward = reward_params["extra_rewards"]["arrival_reward"]
            collision_penalty = reward_params["extra_rewards"]["collision_penalty"]
            self.reward_components.append(TerminalReward("terminal_reward", active["terminal_reward"], arrival_reward, collision_penalty))

    def reset(self):
        # 1. é‡ç½®ä»¿çœŸç¯å¢ƒ
        logging.info("ä»¿çœŸç¯å¢ƒé‡ç½® ...")
        self.sim.reset()
        # 2. é‡ç½®è®¡æ•°å™¨
        self.step_count = 0
        # 3. é‡ç½®åˆå§‹ä½ç½®ã€ç›®æ ‡ä½ç½®
        self.sim.drone.target_position = self.generate_target_positions()
        self.sim.drone.set_orientation()
        # 3. è·å–åˆå§‹è§‚æµ‹
        obs = self.get_obs()
        # åˆå§‹åŒ–æ¯ä¸ªç»„ä»¶çš„å›åˆç´¯è®¡å¥–åŠ±
        self.episode_total_reward = 0
        self.episode_component_rewards = {comp.name: 0.0 for comp in self.reward_components}
        return obs
    
    def step(self, action:np.ndarray):
        self.step_count += 1

        # === 1. æ–½åŠ åŠ¨ä½œå¹¶æ¨è¿›ä»¿çœŸ ===
        action = action.squeeze()
        velocity = self.compute_velocity_from_action(action)
        is_collided, nearest_info = self.sim.step(velocity, self._action_repeat)
        is_arrived = self.check_arrived()
        is_timeout = self.step_count >= self._max_episode_steps
        
        # === 2. çŠ¶æ€åˆ¤æ–­ ===
        done = is_collided or is_arrived or is_timeout

        self.sim.drone.set_orientation()

        # === 3. è§‚æµ‹ ===
        obs = self.get_obs()

       # === 4. å¥–åŠ±è®¡ç®—ï¼ˆåŸºäºå¥–åŠ±ç»„ä»¶ç³»ç»Ÿï¼‰===
        total_reward, component_rewards = self.get_reward(obs, is_arrived, is_collided)  # get_rewardè¿”å›æ€»å¥–åŠ± + å­é¡¹å¥–åŠ±å­—å…¸

        # === 6. é™„åŠ infoè¿”å›ï¼ˆåŒ…æ‹¬ç»†ç²’åº¦å¥–åŠ±ç»Ÿè®¡ï¼‰ ===
        info = dict(
            step_count=self.step_count,
            done=done,
            collision=is_collided,
            arrival=is_arrived,
            timeout=is_timeout,
        )

        for name, reward in component_rewards.items():
            self.episode_component_rewards[name] += reward  # ç´¯åŠ æ¯ä¸ª reward component
        self.episode_total_reward += sum(component_rewards.values())  # æ€»å¥–åŠ±ç´¯è®¡

        if done:
            # å°† episode ç´¯ç§¯å¥–åŠ±åŠ å…¥ infoï¼ˆè¿™æ ·å¤–éƒ¨å¯ä»¥è®¿é—®åˆ°å®ƒï¼‰
            info["episode/total_reward"] = self.episode_total_reward
            for name, total in self.episode_component_rewards.items():
                info[f"episode/{name}"] = total

        return obs, total_reward, done, info    

    def generate_target_positions(self):
        while True:
            # ä» scene_region ä¸­é‡‡æ ·ä½ç½®
            x = np.random.uniform(self.sim.scene_region["x_min"], self.sim.scene_region["x_max"])
            y = np.random.uniform(self.sim.scene_region["y_min"], self.sim.scene_region["y_max"])
            z = np.random.uniform(self.sim.scene_region["z_min"], self.sim.scene_region["z_max"])
            target_position = [x, y, z]
            
            # æ£€æŸ¥ä½ç½®æ˜¯å¦ä¸éšœç¢ç‰©ç¢°æ’
            is_collided, _ = self.sim.drone.check_collision(threshold=10.0)
            if not is_collided:
                logging.info("ğŸš ç›®æ ‡ä½ç½®å®‰å…¨ï¼Œæ— ç¢°æ’")
                return target_position  # å¦‚æœæ²¡æœ‰ç¢°æ’ï¼Œè¿”å›å½“å‰ç”Ÿæˆçš„ä½ç½®
            else:
                logging.warning("ğŸš¨ ç›®æ ‡ä½ç½®ä¸éšœç¢ç‰©å‘ç”Ÿç¢°æ’ï¼Œé‡æ–°ç”Ÿæˆä½ç½®")    
            
    def get_obs(self):
        """
        è·å–å½“å‰æ— äººæœºçš„åŠ¨æ€è§‚æµ‹ï¼Œæ ¹æ®éœ€æ±‚é€‰æ‹©è§‚æµ‹ç‰¹å¾ã€‚
        
        è¿”å›ï¼š
            np.array: æ‹¼æ¥åçš„è§‚æµ‹æ•°æ®
        """

        # è·å–æ·±åº¦å›¾ä¿¡æ¯ï¼ˆå‰æ–¹éšœç¢ç‰©è·ç¦»ï¼‰æ¯ä¸ªåƒç´ æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œä»‹äº [0,1] ä¹‹é—´
        # é è¿‘ç›¸æœºçš„ç‰©ä½“ â†’ æ·±åº¦å€¼æ¥è¿‘0
        # è¿œç¦»ç›¸æœºçš„ç‰©ä½“ â†’ æ·±åº¦å€¼æ¥è¿‘1
        # å¦‚æœçœ‹å‘ç©ºæ— ä¸€ç‰©çš„åœ°æ–¹ï¼Œæ·±åº¦å€¼è¶‹è¿‘äº far
        # æ˜¯äºŒç»´çŸ©é˜µï¼Œæ¯”å¦‚ shape = (240, 320)
        depth_image = self.sim.drone.get_depth_image()
        if "grid_shape" in self.env_params.get("observation", {}):
            grid_shape = self.env_params["observation"]["grid_shape"]
            grid_shape_tuple = ast.literal_eval(grid_shape)
        else:
            grid_shape_tuple = (4,4)
        obs = self.pool_depth_image(depth_image, grid_shape_tuple)
        flatten_obs = obs.flatten()

        # æ‹¼æ¥å½“å‰æ— äººæœºçš„çŠ¶æ€ä¿¡æ¯å’Œæ·±åº¦å›¾ä¿¡æ¯

        return flatten_obs
    
    def get_reward(self, obs, is_arrived, is_collided):
        """
        è®¡ç®—å½“å‰æ— äººæœºçš„å¥–åŠ±å€¼ï¼Œç»„ä»¶åŒ–ç®¡ç†
        è¿”å›:
            total_reward: ç»¼åˆå¥–åŠ±
            component_rewards: æ¯ä¸ªå­å¥–åŠ±é¡¹
        """
        total_reward = 0.0
        component_rewards = {}

        for component in self.reward_components:
            reward = component.compute(self, obs=obs, is_arrived=is_arrived, is_collided=is_collided)
            weighted_reward = reward * component.weight
            total_reward += weighted_reward
            component_rewards[component.name] = weighted_reward

        return total_reward, component_rewards

    def check_arrived(self,arrival_threshold=5.0):
        """
        æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡ç‚¹é™„è¿‘ã€‚

        å‚æ•°ï¼š
            current_position: å½“å‰æ— äººæœºçš„ä½ç½® (x, y, z)
            target_position: ç›®æ ‡ä½ç½® (x, y, z)
            arrival_threshold: åˆ°è¾¾ç›®æ ‡çš„è·ç¦»é˜ˆå€¼
        
        è¿”å›ï¼š
            bool: å¦‚æœåˆ°è¾¾ç›®æ ‡é™„è¿‘ï¼Œè¿”å› Trueï¼›å¦åˆ™è¿”å› False
        """
        distance_to_target = np.linalg.norm(np.array(self.sim.drone.state.position) - np.array(self.sim.drone.target_position))
        return distance_to_target <= arrival_threshold  # å¦‚æœè·ç¦»å°äºé˜ˆå€¼ï¼Œè®¤ä¸ºåˆ°è¾¾ç›®æ ‡

    def compute_velocity_from_action(self, action: np.ndarray):
        """
        æ ¹æ®æŒ‡å®š mode è§£é‡ŠåŠ¨ä½œï¼Œå¹¶æ‰§è¡Œå¯¹åº”æ§åˆ¶ã€‚

        å‚æ•°:
            action (np.ndarray): åŠ¨ä½œå‘é‡
            mode (str): æ§åˆ¶æ¨¡å¼ï¼Œå¯ä¸º 'cartesian', 'spherical', 'adjust'
        """
        mode = self.env_params["action"]["type"]
        
        if mode == "cartesian":
            new_velocity = np.array(action, dtype=np.float32)

        elif mode == "spherical":
            # ç»å¯¹çƒåæ ‡ â†’ ç¬›å¡å°”
            v, theta, phi = action
            vx = v * np.sin(theta) * np.cos(phi)
            vy = v * np.sin(theta) * np.sin(phi)
            vz = v * np.cos(theta)
            new_velocity = np.array([vx, vy, vz], dtype=np.float32)

        elif mode == "adjust":
            delta_theta = action[0]
            delta_phi   = action[1]

            # ç›®æ ‡é€Ÿåº¦è®¾å®š
            v_horiz = 15.0  # æ°´å¹³é€Ÿåº¦
            v_vert = 5.0    # å‚ç›´é€Ÿåº¦
            # è·å–å½“å‰ä½ç½®ä¸ç›®æ ‡ä½ç½®
            current_position = np.array(self.sim.drone.state.position)
            target_position = np.array(self.sim.drone.target_position)

            # ç”¨ç›®æ ‡æ–¹å‘æ›¿ä»£å½“å‰é€Ÿåº¦æ–¹å‘
            direction_vector = target_position - current_position
            norm = np.linalg.norm(direction_vector)

            if norm < 1e-3:
                theta = np.pi / 2
                phi = 0.0
            else:
                # è®¡ç®—ä»å½“å‰ä½ç½®æŒ‡å‘ç›®æ ‡ä½ç½®çš„æ–¹å‘è§’
                theta = np.arccos(direction_vector[2] / norm)  # æè§’ï¼ˆä¿¯ä»°ï¼‰
                phi = np.arctan2(direction_vector[1], direction_vector[0])  # æ–¹ä½è§’ï¼ˆåèˆªï¼‰

            theta_new = np.clip(theta + delta_theta, 0, np.pi)
            phi_new = phi + delta_phi

            # æ„é€ å•ä½æ–¹å‘å‘é‡ï¼ˆæ–¹å‘ç¡®å®šï¼Œä½†æ¨¡é•¿ä¸é€Ÿåº¦æ— å…³ï¼‰
            vx_unit = np.sin(theta_new) * np.cos(phi_new)
            vy_unit = np.sin(theta_new) * np.sin(phi_new)
            vz_unit = np.cos(theta_new)

            # å½’ä¸€åŒ–æ°´å¹³åˆ†é‡å‘é‡
            horiz_norm = np.linalg.norm([vx_unit, vy_unit])
            if horiz_norm < 1e-6:
                vx = 0.0
                vy = 0.0
            else:
                vx = v_horiz * (vx_unit / horiz_norm)
                vy = v_horiz * (vy_unit / horiz_norm)

            # å‚ç›´é€Ÿåº¦ç›´æ¥è®¾ä¸ºå›ºå®šæ¨¡é•¿ï¼ˆæ–¹å‘ç”± theta_new å†³å®šï¼‰
            vz = v_vert * np.sign(vz_unit)

            new_velocity = np.array([vx, vy, vz], dtype=np.float32)
        
        elif mode == "discrete_adjust":
            delta_theta, delta_phi = self.action_idx_to_delta[action]
            
            # # ç›®æ ‡é€Ÿåº¦è®¾å®š
            speed = 15.0
            # è·å–å½“å‰ä½ç½®ä¸ç›®æ ‡ä½ç½®
            current_position = np.array(self.sim.drone.state.position)
            target_position = np.array(self.sim.drone.target_position)

            # ç”¨ç›®æ ‡æ–¹å‘æ›¿ä»£å½“å‰é€Ÿåº¦æ–¹å‘
            direction_vector = target_position - current_position
            norm = np.linalg.norm(direction_vector)

            if norm < 1e-3:
                theta = np.pi / 2
                phi = 0.0
            else:
                # è®¡ç®—ä»å½“å‰ä½ç½®æŒ‡å‘ç›®æ ‡ä½ç½®çš„æ–¹å‘è§’
                theta = np.arccos(direction_vector[2] / norm)  # æè§’ï¼ˆä¿¯ä»°ï¼‰
                phi = np.arctan2(direction_vector[1], direction_vector[0])  # æ–¹ä½è§’ï¼ˆåèˆªï¼‰

            theta_new = np.clip(theta + delta_theta, 0, np.pi)
            phi_new = phi + delta_phi

            # æ„é€ å•ä½æ–¹å‘å‘é‡ï¼ˆæ–¹å‘ç¡®å®šï¼Œä½†æ¨¡é•¿ä¸é€Ÿåº¦æ— å…³ï¼‰
            vx_unit = np.sin(theta_new) * np.cos(phi_new)
            vy_unit = np.sin(theta_new) * np.sin(phi_new)
            vz_unit = np.cos(theta_new)

            # è®¡ç®—é€Ÿåº¦å‘é‡ï¼ˆå•ä½å‘é‡ä¹˜ä»¥é€Ÿåº¦å¤§å°ï¼‰
            vx = speed * vx_unit
            vy = speed * vy_unit
            vz = speed * vz_unit

            new_velocity = np.array([vx, vy, vz], dtype=np.float32)
        
        elif mode == "discrete_adjust_2":
            delta_theta, delta_phi = self.action_idx_to_delta[action]
            
            # # ç›®æ ‡é€Ÿåº¦è®¾å®š
            v_horiz = 15.0  # æ°´å¹³é€Ÿåº¦
            v_vert = 5.0    # å‚ç›´é€Ÿåº¦
            # è·å–å½“å‰ä½ç½®ä¸ç›®æ ‡ä½ç½®
            current_position = np.array(self.sim.drone.state.position)
            target_position = np.array(self.sim.drone.target_position)

            # ç”¨ç›®æ ‡æ–¹å‘æ›¿ä»£å½“å‰é€Ÿåº¦æ–¹å‘
            direction_vector = target_position - current_position
            norm = np.linalg.norm(direction_vector)

            if norm < 1e-3:
                theta = np.pi / 2
                phi = 0.0
            else:
                # è®¡ç®—ä»å½“å‰ä½ç½®æŒ‡å‘ç›®æ ‡ä½ç½®çš„æ–¹å‘è§’
                theta = np.arccos(direction_vector[2] / norm)  # æè§’ï¼ˆä¿¯ä»°ï¼‰
                phi = np.arctan2(direction_vector[1], direction_vector[0])  # æ–¹ä½è§’ï¼ˆåèˆªï¼‰

            theta_new = np.clip(theta + delta_theta, 0, np.pi)
            phi_new = phi + delta_phi

            # æ„é€ å•ä½æ–¹å‘å‘é‡ï¼ˆæ–¹å‘ç¡®å®šï¼Œä½†æ¨¡é•¿ä¸é€Ÿåº¦æ— å…³ï¼‰
            vx_unit = np.sin(theta_new) * np.cos(phi_new)
            vy_unit = np.sin(theta_new) * np.sin(phi_new)
            vz_unit = np.cos(theta_new)

            # å½’ä¸€åŒ–æ°´å¹³åˆ†é‡å‘é‡
            horiz_norm = np.linalg.norm([vx_unit, vy_unit])
            if horiz_norm < 1e-6:
                vx = 0.0
                vy = 0.0
            else:
                vx = v_horiz * (vx_unit / horiz_norm)
                vy = v_horiz * (vy_unit / horiz_norm)

            # å‚ç›´é€Ÿåº¦ç›´æ¥è®¾ä¸ºå›ºå®šæ¨¡é•¿ï¼ˆæ–¹å‘ç”± theta_new å†³å®šï¼‰
            vz = v_vert * np.sign(vz_unit)

            new_velocity = np.array([vx, vy, vz], dtype=np.float32)
        
        

        elif mode in ["horizon_discrete_adjust_3", "horizon_discrete_adjust_5", "horizon_discrete_adjust_7"]:
            delta_phi = self.action_idx_to_delta[action]

            # ç›®æ ‡é€Ÿåº¦è®¾å®š
            speed = 15.0
            # è·å–å½“å‰ä½ç½®ä¸ç›®æ ‡ä½ç½®
            current_position = np.array(self.sim.drone.state.position)
            target_position = np.array(self.sim.drone.target_position)

            # ç”¨ç›®æ ‡æ–¹å‘æ›¿ä»£å½“å‰é€Ÿåº¦æ–¹å‘
            direction_vector = target_position - current_position
            norm = np.linalg.norm(direction_vector)

            if norm < 1e-3:
                theta = np.pi / 2
                phi = 0.0
            else:
                # è®¡ç®—ä»å½“å‰ä½ç½®æŒ‡å‘ç›®æ ‡ä½ç½®çš„æ–¹å‘è§’
                theta = np.arccos(direction_vector[2] / norm)  # æè§’ï¼ˆä¿¯ä»°ï¼‰
                phi = np.arctan2(direction_vector[1], direction_vector[0])  # æ–¹ä½è§’ï¼ˆåèˆªï¼‰

            theta_new = theta
            phi_new = phi + delta_phi

            # æ„é€ å•ä½æ–¹å‘å‘é‡ï¼ˆæ–¹å‘ç¡®å®šï¼Œä½†æ¨¡é•¿ä¸é€Ÿåº¦æ— å…³ï¼‰
            vx_unit = np.sin(theta_new) * np.cos(phi_new)
            vy_unit = np.sin(theta_new) * np.sin(phi_new)
            vz_unit = np.cos(theta_new)

            # è®¡ç®—é€Ÿåº¦å‘é‡ï¼ˆå•ä½å‘é‡ä¹˜ä»¥é€Ÿåº¦å¤§å°ï¼‰
            vx = speed * vx_unit
            vy = speed * vy_unit
            vz = speed * vz_unit

            new_velocity = np.array([vx, vy, vz], dtype=np.float32)

        else:
            raise ValueError(f"Unsupported action mode: '{mode}'. Expected 'cartesian', 'spherical', or 'adjust'.")
             
        return new_velocity

    def pool_depth_image(self, depth_image, grid_shape=(4, 4)):
        """
        å¯¹æ·±åº¦å›¾è¿›è¡Œæœ€å°æ± åŒ–ï¼ŒæŒ‰ç½‘æ ¼åˆ’åˆ†ã€‚
        å‚æ•°:
            depth_image: np.ndarray, shape=(H, W)
            grid_shape: tuple, (rows, cols)
        è¿”å›:
            pooled: np.ndarray, shape=(rows, cols)
        """
        assert isinstance(depth_image, np.ndarray), "Input must be a NumPy array"
        assert depth_image.ndim == 2, f"Expected 2D array, got {depth_image.shape}"

        H, W = depth_image.shape
        rows, cols = grid_shape
        h_step, w_step = H // rows, W // cols

        pooled = np.empty((rows, cols), dtype=depth_image.dtype)

        for i in range(rows):
            for j in range(cols):
                region = depth_image[i*h_step:(i+1)*h_step, j*w_step:(j+1)*w_step]
                pooled[i, j] = np.min(region)

        return pooled
