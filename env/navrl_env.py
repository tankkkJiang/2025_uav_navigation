# coding: utfâ€‘8
"""
env/navrl_env.py
NavRL ç¯å¢ƒï¼ˆé™æ€ + åŠ¨æ€éšœç¢ï¼Œè¿ç»­ 3D é€Ÿåº¦æ§åˆ¶ï¼‰
ä¸ sim.world.World è§£è€¦ï¼Œæ‰€æœ‰ç‰©ç†ç»†èŠ‚äº¤ç»™ Worldã€‚
"""
from typing import Dict, Tuple, List
import gym
from gym import spaces
import numpy as np
import logging
import ast

from sim.world import World
from env.wrappers.navrl_reward_wrapper import (
    VelReward, StaticSafetyReward, DynamicSafetyReward,
    SmoothReward, HeightPenalty, TerminalReward
)

# -------------------------------------------------------------

class NavRLEnv(gym.Env):
    """
    è§‚æµ‹ï¼š
        S_int âˆˆ R7  +  S_dyn (NdÃ—M)  +  S_stat (NhÃ—Nv) â†’ å±•å¹³æˆ 1 ç»´
    åŠ¨ä½œï¼š
        \hat V_ctrl^G âˆˆ [0,1]^3 ï¼ˆç›®æ ‡åæ ‡ç³»ä¸‹å½’ä¸€åŒ–æ§åˆ¶å‘é‡ï¼‰ï¼Œåé¢å®ç° hat_vg â†’ v_g â†’ v_w
    """
    metadata = {"render_modes": []}

    def __init__(self, cfg: Dict):
        super().__init__()
        self.cfg = cfg
        self._build_sim()
        self._build_spaces()
        self._build_rewards()

        self.max_steps = cfg["episode"]["max_episode_timesteps"]
        self.action_repeat = cfg["episode"]["action_repeat"]
        self.step_cnt = 0
        self.prev_vel = np.zeros(3, dtype=np.float32)

    # ------------------------------------------------------------------
    # åˆå§‹åŒ–
    # ------------------------------------------------------------------
    def _build_sim(self):
        scene_cfg   = self.cfg["scene"]
        drone_cfg   = self.cfg["drone"]
        voxel_size  = scene_cfg.get("voxel", {}).get("size", None)
        self.world  = World(
            use_gui      = self.cfg["use_gui"],
            scene_type   = scene_cfg["type"],            # "navrl"
            scene_region = scene_cfg["region"],
            obstacle_params = scene_cfg["obstacle"],
            drone_params = drone_cfg,
            voxel_size   = voxel_size
        )

    def _build_spaces(self):
        # ----- åŠ¨ä½œ -----
        self.v_lim = self.cfg["action"]["v_limit"]
        self.action_space = spaces.Box(low=np.float32([0, 0, 0]),
                                       high=np.float32([1, 1, 1]),
                                       dtype=np.float32)

        # ----- è§‚æµ‹ -----
        obs_cfg = self.cfg["observation"]
        int_dim    = 7
        dyn_dim    = obs_cfg["num_dyn"] * obs_cfg["dyn_feat_dim"]
        stat_dim   = obs_cfg["num_h"] * obs_cfg["num_v"]
        self.obs_dim = int_dim + dyn_dim + stat_dim
        self.observation_space = spaces.Box(
            low  = -np.inf, high = np.inf,
            shape=(self.obs_dim,), dtype=np.float32
        )

    def _build_rewards(self):
        r_cfg = self.cfg["reward"]
        self.reward_components = [
            VelReward               ("vel"    , r_cfg["weights"]["vel"]),
            StaticSafetyReward      ("ss"     , r_cfg["weights"]["ss"]),
            DynamicSafetyReward     ("ds"     , r_cfg["weights"]["ds"],
                                      num_dyn=self.cfg["observation"]["num_dyn"]),
            SmoothReward            ("smooth" , r_cfg["weights"]["smooth"]),
            HeightPenalty           ("height" , r_cfg["weights"]["height"]),
            TerminalReward          ("terminal",
                                     success_r = r_cfg["success_reward"],
                                     collision_p = r_cfg["collision_penalty"])
        ]
        self.comp_total = {c.name: 0.0 for c in self.reward_components}

    # ------------------------------------------------------------------
    # Gym å¿…å¤‡æ¥å£
    # ------------------------------------------------------------------
    def reset(self, *, seed=None, options=None):
        super().reset(seed=seed)
        self.world.reset()
        self.step_cnt = 0
        self.prev_vel = np.zeros(3, dtype=np.float32)

        # éšæœºç›®æ ‡
        self.start_pos  = np.array(self.world.drone.state.position)
        self.goal_pos   = self._sample_goal()

        # åœ¨ç›®æ ‡/ä¸–ç•Œåæ ‡ç³»ä¹‹é—´æ„é€ æ—‹è½¬çŸ©é˜µ
        gx    = self.goal_pos - self.start_pos
        gx[2] = 0.0
        gx   /= (np.linalg.norm(gx) + 1e-6)            # e1
        gz    = np.array([0, 0, 1], dtype=np.float32)  # e3
        gy    = np.cross(gz, gx)                       # e2
        self.R_g2w = np.stack([gx, gy, gz], axis=1)    # 3Ã—3
        self.R_w2g = self.R_g2w.T

        # è®°å½•ç›®æ ‡åœ¨ G åæ ‡ä¸­çš„ x åæ ‡ï¼ˆå…¶ä½™ä¸º 0ï¼‰
        self.goal_dist = np.linalg.norm(self.goal_pos - self.start_pos)

        obs = self._get_obs()  # åæ ‡ç³»å°±ç»ªåå†å–è§‚æµ‹
        self.comp_total = {c.name: 0.0 for c in self.reward_components}

        return obs, {}

    def step(self, action: np.ndarray):
        # å½’ä¸€åŒ– â†’ çœŸå®é€Ÿåº¦ï¼ˆç›®æ ‡åæ ‡ç³»ï¼‰â†’ ä¸–ç•Œåæ ‡ç³»
        hat_vg = np.clip(action.astype(np.float32), 0.0, 1.0)
        v_g    = self.v_lim * (2.0 * hat_vg - 1.0)          # [-v_lim, v_lim]
        v_w    = self.R_g2w @ v_g                            # 3D ä¸–ç•Œå‘é‡
        # æ¨è¿›ç‰©ç†
        collided, _ = self.world.step(v_w, num_steps=self.action_repeat)

        self.step_cnt += 1
        arrived  = self.check_arrived()
        timeout  = self.step_cnt >= self.max_steps
        done = collided or arrived or timeout

        obs = self._get_obs()
        reward, comp_rs = self._calc_reward(obs, arrived, collided, v_g)

        info = {
            "arrival": arrived,
            "collision": collided,
            "timeout": timeout
        }
        # ç»Ÿè®¡æ¯é›†å¥–åŠ±
        for k, v in comp_rs.items():
            self.comp_total[k] += v
            info[f"episode/{k}"] = self.comp_total[k]

        self.prev_vel = v_g  # åœ¨ç›®æ ‡åæ ‡ç³»å†…ç®— smooth
        return obs, reward, done, info

    # ------------------------------------------------------------------
    # å†…éƒ¨å·¥å…·
    # ------------------------------------------------------------------
    # ç›®æ ‡ç”Ÿæˆï¼ˆ50mÃ—50mï¼‰
    def _sample_goal(self):
        """
        ç›®æ ‡ç”Ÿæˆé€»è¾‘ï¼š
        â€¢ ç©ºé—´ä¸Šï¼šåœ¨ scene.region ç«‹æ–¹ä½“å†…éšæœºé‡‡æ ·
        â€¢ å®‰å…¨ä¸Šï¼šç¡®ä¿ç›®æ ‡ä¸ä»»æ„éšœç¢ä¿æŒ â‰¥10 m è·ç¦»ï¼Œå¦åˆ™é‡é‡‡
        """
        region = self.world.scene_region
        while True:
            x = np.random.uniform(region["x_min"], region["x_max"])
            y = np.random.uniform(region["y_min"], region["y_max"])
            z = np.random.uniform(region["z_min"], region["z_max"])
            self.world.drone.target_position = [x, y, z]

            # æ£€æŸ¥ä¸éšœç¢çš„è·ç¦»é˜ˆå€¼ï¼ˆ10mï¼‰
            is_collided, _ = self.world.drone.check_collision(threshold=10.0)
            if not is_collided:
                logging.info("ğŸš ç›®æ ‡ä½ç½®å®‰å…¨ï¼Œæ— ç¢°æ’")
                return np.array([x, y, z], dtype=np.float32)
            else:
                logging.warning("ğŸš¨ ç›®æ ‡ä½ç½®ä¸éšœç¢ç‰©è¿‡è¿‘ï¼Œé‡æ–°ç”Ÿæˆ")


    def check_arrived(self, arrival_threshold=5.0):
        """
        æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡ç‚¹é™„è¿‘ã€‚

        å‚æ•°ï¼š
            arrival_threshold: åˆ°è¾¾ç›®æ ‡çš„è·ç¦»é˜ˆå€¼

        è¿”å›ï¼š
            bool: å¦‚æœåˆ°è¾¾ç›®æ ‡é™„è¿‘ï¼Œè¿”å› Trueï¼›å¦åˆ™è¿”å› False
        """
        distance_to_target = np.linalg.norm(
            np.array(self.world.drone.state.position) - np.array(self.world.drone.target_position)
        )
        return distance_to_target <= arrival_threshold  # å¦‚æœè·ç¦»å°äºé˜ˆå€¼ï¼Œè®¤ä¸ºåˆ°è¾¾ç›®æ ‡

    # ------------------------------------------------------------
    # è§‚æµ‹
    # ------------------------------------------------------------
    def _get_obs(self):
        obs_cfg = self.cfg["observation"]

        # ----- ä¸–ç•Œ â†’ ç›®æ ‡åæ ‡ç³» -----
        p_r_w = np.array(self.world.drone.state.position, dtype=np.float32)
        v_r_w = np.array(self.world.drone.state.linear_velocity, dtype=np.float32)
        # ä½ç½®ã€é€Ÿåº¦æ˜ å°„åˆ° G
        p_r_g = self.R_w2g @ (p_r_w - self.start_pos)  # 3,
        v_r_g = self.R_w2g @ v_r_w  # 3,

        # --------- S_int (7)  -----------------------------------
        # ç›®æ ‡åœ¨ G ä¸­æ˜¯ [goal_dist, 0, 0]
        dir_pg_g = np.array([self.goal_dist, 0.0, 0.0], dtype=np.float32) - p_r_g
        dist_pg = np.linalg.norm(dir_pg_g) + 1e-6
        dir_norm = dir_pg_g / dist_pg
        s_int = np.concatenate([dir_norm, [dist_pg], v_r_g], axis=0)  # (7,)

        # --------- S_dyn ---------
        dyn_list = self.world.drone.get_nearest_dynamic_obs(k=obs_cfg["num_dyn"])  # List[dict]
        num_dyn = obs_cfg["num_dyn"]
        feat_dim = obs_cfg["dyn_feat_dim"]

        buf = np.zeros((num_dyn, feat_dim), dtype=np.float32)  # å…¨é›¶åˆå§‹åŒ–
        for i, d in enumerate(dyn_list[:num_dyn]):  # æœ€å¤šå¡« num_dyn æ¡
            rel_w = d["pos"] - p_r_w
            rel_g = self.R_w2g @ rel_w
            dist = np.linalg.norm(rel_g) + 1e-6
            dir_g = rel_g / dist
            vel_g = self.R_w2g @ d["vel"]
            buf[i] = np.concatenate([dir_g, [dist], vel_g, d["size"]], axis=0)

        s_dyn = buf.flatten()

        # --------- S_stat ---------
        rays = self.world.drone.cast_static_rays(
            num_h=obs_cfg["num_h"], num_v=obs_cfg["num_v"],
            max_dist=obs_cfg["max_ray"])
        s_stat = np.array(rays, dtype=np.float32).flatten()

        obs = np.concatenate([s_int, s_dyn, s_stat], axis=0)
        return obs[: self.obs_dim]      # ä¿åº•è£å‰ª

    # ------------------------------------------------------------
    # å¥–åŠ±
    # ------------------------------------------------------------
    def _calc_reward(self, obs, arrived, collided, cur_vel):
        total, comp = 0.0, {}
        for rc in self.reward_components:
            r = rc.compute(self, obs=obs,
                           arrived=arrived, collided=collided,
                           cur_vel=cur_vel, prev_vel=self.prev_vel)
            comp[rc.name] = r
            total += r
        return total, comp